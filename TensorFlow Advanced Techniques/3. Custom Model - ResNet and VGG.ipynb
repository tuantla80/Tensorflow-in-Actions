{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding a Wide and Deep Model\n",
    "\n",
    "In this lab, we'll show how you can implement a wide and deep model. We'll first look at how to build it with the Functional API then show how to encapsulate this into a class. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CmI9MQA6Z72_"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8RKbMogoaHvc"
   },
   "source": [
    "## Build the Model\n",
    "\n",
    "Let's implement the wide and deep model as shown in class. As shown below, the Functional API is very flexible in implementing complex models. \n",
    "- You will specify the previous layer when you define a new layer. \n",
    "- When you define the `Model`, you will specify the inputs and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uz4pA6uEucZ8"
   },
   "outputs": [],
   "source": [
    "# define inputs\n",
    "input_a = Input(shape=[1], name=\"Wide_Input\")\n",
    "input_b = Input(shape=[1], name=\"Deep_Input\")\n",
    "\n",
    "# define deep path\n",
    "hidden_1 = Dense(30, activation=\"relu\")(input_b)\n",
    "hidden_2 = Dense(30, activation=\"relu\")(hidden_1)\n",
    "\n",
    "# define merged path\n",
    "concat = concatenate([input_a, hidden_2])\n",
    "output = Dense(1, name=\"Output\")(concat)\n",
    "\n",
    "# define another output for the deep path\n",
    "aux_output = Dense(1,name=\"aux_Output\")(hidden_2)\n",
    "\n",
    "# build the model\n",
    "model = Model(inputs=[input_a, input_b], outputs=[output, aux_output])\n",
    "\n",
    "# # visualize the architecture\n",
    "# plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement as a Class\n",
    "\n",
    "Alternatively, you can also implement this same model as a class. \n",
    "- For that, you define a class that inherits from the [Model](https://keras.io/api/models/model/) class.\n",
    "- Inheriting from the existing `Model` class lets you use the Model methods such as `compile()`, `fit()`, `evaluate()`. \n",
    "\n",
    "When inheriting from `Model`, you will want to define at least two functions:\n",
    "- `__init__()`: you will initialize the instance attributes.\n",
    "- `call()`: you will build the network and return the output layers.\n",
    "\n",
    "If you compare the two methods, the structure is very similar, except when using the class, you'll define all the layers in one function, `init`, and connect the layers together in another function, `call`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NwyCp57qqdXS"
   },
   "outputs": [],
   "source": [
    "# inherit from the Model base class\n",
    "class WideAndDeepModel(Model):\n",
    "    def __init__(self, units=30, activation='relu', **kwargs):\n",
    "        '''initializes the instance attributes'''\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = Dense(units, activation=activation)\n",
    "        self.hidden2 = Dense(units, activation=activation)\n",
    "        self.main_output = Dense(1)\n",
    "        self.aux_output = Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        '''defines the network architecture'''\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        \n",
    "        return main_output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KVOkjlgwuD_9"
   },
   "outputs": [],
   "source": [
    "# create an instance of the model\n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing Model subclasses: Eg. ResNet\n",
    "In this lab, you will continue exploring Model subclassing by building a more complex architecture. \n",
    "\n",
    "[Residual Networks](https://arxiv.org/abs/1512.03385) make use of skip connections to make deep models easier to train. \n",
    "- There are branches as well as many repeating blocks of layers in this type of network. \n",
    "- You can define a model class to help organize this more complex code, and to make it easier to re-use your code when building the model.\n",
    "- As before, you will inherit from the [Model class](https://keras.io/api/models/model/) so that you can make use of the other built-in methods that Keras provides.\n",
    "\n",
    "As shown in the lectures, you will first implement the Identity Block which contains the skip connections (i.e. the `add()` operation below. This will also inherit the Model class and implement the `__init__()` and `call()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityBlock(tf.keras.Model):\n",
    "    def __init__(self, filters, kernel_size):\n",
    "        super(IdentityBlock, self).__init__(name='')\n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters, kernel_size, padding='same')\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters, kernel_size, padding='same')\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.act = tf.keras.layers.Activation('relu')\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        \n",
    "    def call(self, input_tensor):\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        x = self.add([x, input_tensor])\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, you can build the rest of the ResNet model. \n",
    "- You will call your `IdentityBlock` class two times below and that takes care of inserting those blocks of layers into this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(tf.keras.Model):\n",
    "    def __init__(self, num_classess):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(64, 7, padding='same')\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "        self.act = tf.keras.layers.Activation('relu')\n",
    "        self.max_pool = tf.keras.layers.MaxPool2D(3, 3)\n",
    "        \n",
    "        # Use the Identity blocks that you just defined\n",
    "        self.id1a = IdentityBlock(64, 3)\n",
    "        self.id1b = IdentityBlock(64, 3)\n",
    "        \n",
    "        self.global_pool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.classifier = tf.keras.layers.Dense(num_classess, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        x = self.max_pool(x)\n",
    "        \n",
    "        # insert the identity blocks in the middle of the network\n",
    "        x = self.id1a(x)\n",
    "        x = self.id1b(x)\n",
    "        \n",
    "        x = self.global_pool(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet = ResNet(num_classess=10)\n",
    "# resnet.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# resnet.build(input_shape=(28,28))\n",
    "# resnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "As mentioned before, inheriting the Model class allows you to make use of the other APIs that Keras provides, such as:\n",
    "- training\n",
    "- serialization\n",
    "- evaluation\n",
    "\n",
    "You can instantiate a Resnet object and train it as usual like below:\n",
    "\n",
    "**Note**: If you have issues with training in the Coursera lab environment, you can also run this in Colab using the \"open in colab\" badge link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to normalize the images and return (image, label) pairs.\n",
    "def preprocess(features):\n",
    "    return tf.cast(features['image'], tf.float32) / 255., features['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a ResNet instance with 10 output units for MNIST\n",
    "resnet = ResNet(num_classess=10)\n",
    "resnet.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# load and preprocess the dataset\n",
    "dataset = tfds.load('mnist', split=tfds.Split.TRAIN, data_dir='./data')\n",
    "dataset = dataset.map(preprocess).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert isinstance(dataset, tf.data.Dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "resnet.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = tfds.load('mnist', split=tfds.Split.TEST, data_dir='./test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 9\n",
    "ds = dataset_test.take(num_images)  # Only take a single example\n",
    "\n",
    "for example in ds:  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
    "    #print(list(example.keys()))\n",
    "    image = example[\"image\"]\n",
    "    label = example[\"label\"]\n",
    "    #print('image')\n",
    "    # print(image.shape, 'type = ', type(image))\n",
    "    # print(image.numpy())\n",
    "    #print('label')\n",
    "    # print(label, 'type = ', type(label))\n",
    "    print('label is ', label.numpy())\n",
    "#     print('\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = resnet.predict(ds.map(preprocess).batch(1))\n",
    "print(results)\n",
    "for result in results:\n",
    "    print(result.argmax())  # it is label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing Model subclasses: VGG  \n",
    "<img src=\"vgg.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a generic VGG block (TODO)\n",
    "\n",
    "The VGG Network has blocks of layers, where each block has a varied number of layers.\n",
    "- In order to create blocks of layers that have a customizable number of conv2D layers, you'll define a class `Block`, which can generate a customizable block of layers \n",
    "\n",
    "\n",
    "### `__init__`\n",
    "In the constructor `__init__`, store the conv2D parameters and also define the number of conv2D layers using the parameters passed into `__init__`.\n",
    "- Store the filters, kernel_size, and repetitions as class variables so that they can be used later in the `call` function.\n",
    "- Using a for loop, define a number of Conv2D [Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/) layers, based on the number of `repetitions` desired for this block.\n",
    "    - You can define each conv2D layer using `vars` and string formatting to create conv2D_0, conv2D_1, conv2D_3 etc.\n",
    "    - Set these four parameters of Conv2D:\n",
    "        - filters\n",
    "        - kernel_size\n",
    "        - activation: set this to 'relu'\n",
    "        - padding: set this to 'same' (default pading is 'valid').\n",
    "        \n",
    "- Define the [MaxPool2D](https://keras.io/api/layers/pooling_layers/max_pooling2d/) layer that follows these Conv2D layers. \n",
    "    - Set the following parameters for MaxPool2D:\n",
    "        - pool_size: this will be a tuple with two values.\n",
    "        - strides: this will also be a tuple with two values.\n",
    "\n",
    "### `call`\n",
    "In `call`, you will connect the layers together.\n",
    "- The 0-th conv2D layer, `conv2D_0`, immediately follows the `inputs`.\n",
    "- For conv2D layers 1,2 and onward, you can use a for loop to connect conv2D_1 to conv2D_0, and connect conv2D_2 to conv2D_1, and so on.\n",
    "- After connecting all of the conv2D_i layers, add connect the max_pool layer and return the max_pool layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(tf.keras.Model):\n",
    "    def __init__(self, filters, kernel_size, repetitions, pool_size=2, strides=2):\n",
    "        super(Block, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.repetitions = repetitions\n",
    "        \n",
    "        # Define a conv2D_0, conv2D_1, etc based on the number of repetitions\n",
    "        for i in range(repetitions):\n",
    "            \n",
    "            # Define a Conv2D layer, specifying filters, kernel_size, activation and padding.\n",
    "            vars(self)[f'conv2D_{i}'] = tf.keras.layers.Conv2D(filters, kernel_size, activation='relu', padding='same')\n",
    "            \n",
    "        # Define the max pool layer that will be added after the Conv2D blocks\n",
    "        self.max_pool = tf.keras.layers.MaxPooling2D(pool_size=pool_size, strides=strides)\n",
    "        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # access the class's conv2D_0 layer\n",
    "        conv2D_0 = vars(self)['conv2D_0']\n",
    "        \n",
    "        # Connect the conv2D_0 layer to inputs\n",
    "        x = conv2D_0(inputs)\n",
    "        \n",
    "        # for the remaining conv2D_i layers from 1 to `repetitions` they will be connected to the previous layer\n",
    "        for i in range(1, self.repetitions):\n",
    "            # access conv2D_i by formatting the integer `i`\n",
    "            conv2D_i = vars(self)[f'conv2D_{i}']\n",
    "            \n",
    "            # Use the conv2D_i and connect it to the previous layer\n",
    "            x = conv2D_i(x)\n",
    "            \n",
    "        # Finally, add the max_pool layer\n",
    "        max_pool = self.max_pool(x)\n",
    "        \n",
    "        return max_pool  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.test_block_class(Block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Custom VGG network (TODO)\n",
    "This model stack has a series of VGG blocks, which can be created using the `Block` class that you defined earlier.\n",
    "\n",
    "### `__init__`\n",
    "- Recall that the `__init__` constructor of `Block` takes several function parameters, \n",
    "    - filters, kernel_size, repetitions: you'll set these.\n",
    "    - pool_size and strides: you can use the default values.\n",
    "- For blocks a through e, build the blocks according to the following specifications:\n",
    "- block_a: 64  filters, kernel_size 3, repetitions 2\n",
    "- block_b: 128 filters, kernel_size 3, repetitions 2\n",
    "- block_c: 256 filters, kernel_size 3, repetitions 3\n",
    "- block_d: 512 filters, kernel_size 3, repetitions 3\n",
    "- block_e: 512 filters, kernel_size 3, repetitions 3\n",
    "\n",
    "After block 'e', add the following layers:\n",
    "- flatten: use [Flatten](https://keras.io/api/layers/reshaping_layers/flatten/).\n",
    "- fc: create a fully connected layer using [Dense](https://keras.io/api/layers/core_layers/dense/).  Give this 256 units, and a `'relu'` activation.\n",
    "- classifier: create the classifier using a Dense layer.  The number of units equals the number of classes.  For multi-class classification, use a `'softmax'` activation.\n",
    "\n",
    "### `call`\n",
    "Connect these layers together using the functional API syntax:\n",
    "- inputs\n",
    "- block_a\n",
    "- block_b\n",
    "- block_c\n",
    "- block_d\n",
    "- block_e\n",
    "- flatten\n",
    "- fc\n",
    "- classifier\n",
    "\n",
    "Return the classifier layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVGG(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(MyVGG, self).__init__()\n",
    "\n",
    "        # Creating blocks of VGG with the following \n",
    "        # (filters, kernel_size, repetitions) configurations\n",
    "        self.block_a = Block(filters=64, kernel_size=3, repetitions=2)\n",
    "        self.block_b = Block(filters=128, kernel_size=3, repetitions=2)\n",
    "        self.block_c = Block(filters=256, kernel_size=3, repetitions=3)\n",
    "        self.block_d = Block(filters=512, kernel_size=3, repetitions=3)\n",
    "        self.block_e = Block(filters=512, kernel_size=3, repetitions=3)\n",
    "\n",
    "        # Classification head\n",
    "        # Define a Flatten layer\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # Create a Dense layer with 256 units and ReLU as the activation function\n",
    "        self.fc = tf.keras.layers.Dense(256, activation='relu')\n",
    "        # Finally add the softmax classifier using a Dense layer\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes,activation='softmax')\n",
    "    def call(self, inputs):\n",
    "        # Chain all the layers one after the other\n",
    "        x = self.block_a(inputs)\n",
    "        x = self.block_b(x)\n",
    "        x = self.block_c(x)\n",
    "        x = self.block_d(x)\n",
    "        x = self.block_e(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.test_myvgg_class(MyVGG, Block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and train the VGG network (Optional)\n",
    "You can now load the dataset and proceed to train your VGG network. \n",
    "- This will take a few minutes to complete and is **not required to complete the assignment**.\n",
    "- You can submit your work before starting the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tfds.load('cats_vs_dogs', split=tfds.Split.TRAIN, data_dir='data/')\n",
    "\n",
    "# Initialize VGG with the number of classes \n",
    "vgg = MyVGG(num_classes=2)\n",
    "\n",
    "# Compile with losses and metrics\n",
    "vgg.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess(features):\n",
    "    # Resize and normalize\n",
    "    image = tf.image.resize(features['image'], (224, 224))\n",
    "    return tf.cast(image, tf.float32) / 255., features['label']\n",
    "\n",
    "# Apply transformations to dataset\n",
    "dataset = dataset.map(preprocess).batch(32)\n",
    "\n",
    "# Train the custom VGG model\n",
    "vgg.fit(dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "BasicModel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
